{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d50d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\211369\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset\n",
    "from textbrewer import GeneralDistiller, TrainingConfig, DistillationConfig\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4a6f101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:16<00:00,  8.16s/it]\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"  # You can change to 'cuda' if GPU available\n",
    "\n",
    "teacher_path =  r\"C:\\Users\\211369\\Desktop\\program\\distil\\llama3.2_3b\"\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\n",
    "    teacher_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "# teacher = AutoModelForCausalLM.from_pretrained(teacher_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf538d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_path =  r\"C:\\Users\\211369\\Desktop\\program\\distil\\llama3.2_1b\"\n",
    "student = AutoModelForCausalLM.from_pretrained(student_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f2f48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"\"\"You are a helpful assistant. return awb number, :\n",
    "â— Important:\n",
    "- Do **not** use given examples for any fields\n",
    "- Do **not** use any parts from the prompt as fields\n",
    "- Only return the final JSON object.\n",
    "- Do **not** add any explanation, markdown formatting, or code.\n",
    "- Do **not** include backticks (```) or language tags like ```json.\n",
    "- Do **not** generate Python or any other code.\n",
    "- If data is missing, return `null`, but do not fabricate.\n",
    "- Output must be valid and clean JSON.\n",
    "- Only take values from the given mail\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "Subject: {subject}\n",
    "From: {from_}\n",
    "To: {to}\n",
    "Body:{body}\n",
    "######################################################\n",
    " You are assisting the Airline Cargo team in extracting specific business-critical entities from customer emails. Each email consists of a subject and body, both delimited by triple backticks (```).\n",
    " \n",
    "You must return a JSON array of dictionaries, each representing one AWB (Air Waybill) entry with its corresponding entities. The structure should follow these rules:\n",
    "Return a JSON array of dictionaries.\n",
    "Each dictionary must include the extracted AWB and its associated entities.\n",
    "Omit any fields not found in the email; DO NOT assume values.\n",
    " \n",
    "The final output must be pure JSON: no explanations, no backticks, no extra text.\n",
    "The json output should appear as per the following format\n",
    "[\n",
    "    {{\n",
    "        \"AWB\": \"\",\n",
    "        \"FlightNo\": \"\",\n",
    "        \"Departure-date\": \"\",\n",
    "        \"total-pieces\": ,\n",
    "        \"pieces@dimensions\": [\"\"],\n",
    "        \"dimension-unit\": [\"\"],\n",
    "        \"Weight\": ,\n",
    "        \"weight-unit\": \"\",\n",
    "        \"special-instruction\": \"\",\n",
    "        \"commodity-description\": \"\",\n",
    "        \"product-code\": \"\",\n",
    "        \"Source\": \"\",\n",
    "        \"Destination\": \"\"\n",
    "    }}\n",
    "]\n",
    "AWB (Air Waybill):\n",
    "Must be 11-digit numbers starting with valid airline prefixes: <AWB_PREFIX>.\n",
    "May be referred to as \"MAWB\" or \"GUIA\".\n",
    "Remove hyphens or spaces.\n",
    "One dictionary per AWB; multiple AWBs = multiple dictionaries.\n",
    " \n",
    "FlightNo:\n",
    "Must start with valid carrier codes: <AIRLINE_PREFIX>.\n",
    "Format: airline code + number\n",
    "do not take the date value for the flight number if there is flight date attached with flight number (eg KE706/18APR in this only take KE706)\n",
    "if no values are found keep as null\n",
    " \n",
    "Departure-date:\n",
    "Extract in YYYY-MM-DD format.\n",
    "If given as a range like 23/24/07, choose the latest date (i.e., 2025-07-24).\n",
    "If given as a relative day (e.g., \"next Monday\"), assume today's date is 2025-03-22 (Saturday) and resolve accordingly.\n",
    " \n",
    "total-pieces:\n",
    "Integer value representing total cargo pieces.\n",
    " \n",
    "pieces@dimensions:\n",
    "Format: list like [\"2@24x17x9\"].\n",
    "May appear as pcs x l x b x h or pcs @ l x b x h.\n",
    "Extract all combinations; prioritize individual dimensions over total.\n",
    " \n",
    "dimension-unit:\n",
    "Supported units: \"CM\", \"M\", \"IN\", \"OTH\".\n",
    "Provide as a list matching the sequence of pieces@dimensions.\n",
    " \n",
    "Weight:\n",
    "If individual weights are given, compute the total.\n",
    "Use chargeable weight (CW) or gross weight (G/W) if explicitly mentioned.\n",
    "If weight is embedded in piece-dimension combos, extract accordingly.\n",
    " \n",
    "weight-unit:\n",
    "Supported values: \"KG\", \"KGS\", \"LBS\", \"OTH\".\n",
    " \n",
    "special-instruction:\n",
    "Extract any special handling notes.\n",
    "Always translate to English.\n",
    " \n",
    "commodity-description:\n",
    "Free text describing the goods.\n",
    "Always translate to English.\n",
    " \n",
    "product-code:\n",
    "If not explicitly given, infer from commodity description:\n",
    "\"GEN\" for general cargo\n",
    "\"HAZ\" for hazardous materials\n",
    "\"DG\" for dangerous goods\n",
    " \n",
    "Source / Destination:\n",
    "Extract from IATA codes in formats like EWR-OME (EWR = Source, OME = Destination).\n",
    "Do not assume source location from senderâ€™s location or from flight number.\n",
    " \n",
    "Must Translate all extracted text into  json.\n",
    "Do not fabricate missing values.\n",
    "Always return a clean JSON output only â€” no markdown, no backticks, no wrapping text.\n",
    "Must Not generate anything other than the base json file. Do NOT generate any code.\n",
    "Only the output is required do not generate anything else\n",
    " \n",
    "since there is a json format given generate just as the json format. Do not generate in loop. if it start to generate in loop stop the generation.\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d90b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptResponseDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, prompt_template, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_template = prompt_template  # <-- store the prompt\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for item in data:\n",
    "            subject = item.get(\"subject\", \"\")\n",
    "            from_ = item.get(\"from\", \"\")\n",
    "            to = item.get(\"to\", \"\")\n",
    "            body = item.get(\"body\", \"\")\n",
    "\n",
    "            self.samples.append({\n",
    "                \"subject\": subject,\n",
    "                \"from_\": from_,\n",
    "                \"to\": to,\n",
    "                \"body\": body\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        subject = sample[\"subject\"]\n",
    "        from_ = sample[\"from_\"]\n",
    "        to = sample[\"to\"]\n",
    "        body = sample[\"body\"]\n",
    "\n",
    "        # Use the shared prompt template\n",
    "        prompt = self.prompt_template.format(subject=subject, from_=from_, to=to, body=body)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": inputs[\"input_ids\"].squeeze(0).clone(),\n",
    "            \"prompt\": prompt,\n",
    "            \"subject\": subject,\n",
    "            \"from_\": from_,\n",
    "            \"to\": to,\n",
    "            \"body\": body\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e11136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\211369\\Desktop\\program\\structured_outputs.json\"\n",
    "dataset = PromptResponseDataset(file_path, tokenizer, prompt_template)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8d66c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¨ Prompt:\n",
      " You are a helpful assistant. return awb number, :\n",
      "â— Important:\n",
      "- Do **not** use given examples for any fields\n",
      "- Do **not** use any parts from the prompt as fields\n",
      "- Only return the final JSON object.\n",
      "- Do **not** add any explanation, markdown formatting, or code.\n",
      "- Do **not** include backticks (```) or language tags like ```json.\n",
      "- Do **not** generate Python or any other code.\n",
      "- If data is missing, return `null`, but do not fabricate.\n",
      "- Output must be valid and clean JSON.\n",
      "- Only take values from the given mail\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Subject: [External] <AR> : EY9557/ 11FEB2024 DXBAUH\n",
      "From: Shaheen Shafeek <SShafeek@etihad.ae>\n",
      "To: \n",
      "Body:ATTENTION: This email originated from a source outside of our organization. Please ensure that you recognize the sender and the content is safe before you open any attachments or click any links.\n",
      "EY BOOKING\n",
      "Please note below booking is not ready to join subject trucking, please advise Shipper to approach DNATA and complete Security formalities.\n",
      "Booking is now QUED in SPRINT\n",
      "Shaheen Shafeek\n",
      "Cargo Handling Officer U.A.E Northern Emirates\n",
      "Cargo Operations & Delivery\n",
      "M:\n",
      "+971 50 818 0631\n",
      "E:\n",
      "sshafeek@etihad.ae\n",
      "etihadcargo.com\n",
      "From:\n",
      "DXB - Cargo RFS Operations <dxb.cargoRFS@dnata.com>\n",
      "Sent:\n",
      "Sunday, February 11, 2024 16:27\n",
      "To:\n",
      "Cargo Handling DXB <cargohandling_DXB@etihad.ae>; Load Plan DXB <Loadplandxb@dnata.com>; dnata Courier <dnatacourier@dnata.com>; dn ULD - WH & Trucking (OAL) <dnULDOALWT@dnata.com>; dn ULD - BGE & Empty (OAL) <dnULDOALBE@dnata.com>; dnata UCM & SCM\n",
      " <dnataUCM&SCM@dnata.com>\n",
      "Cc:\n",
      "'MICCO RFS (RFS@miccologistics.com)' <RFS@miccologistics.com>; Cargo Handling DXB <cargohandling_DXB@etihad.ae>; Shaheen Shafeek <SShafeek@etihad.ae>; Sandy Marcelino Santos <sandy.santos@dnata.com>\n",
      "Subject:\n",
      "RE: RFS LOAD PLAN : EY9557/ 11FEB2024 DXBAUH - VER: 1\n",
      "This email comes from an\n",
      "EXTERNAL\n",
      "source.\n",
      "Dear Sangeeth,\n",
      "Refer below shipments status, and kindly advise.\n",
      "607- 28027171Â Â Â  DXBÂ Â Â Â Â Â Â  MLEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  1Â Â Â Â Â Â Â Â Â Â Â Â Â  184Â Â Â Â Â Â Â Â  1.71Â Â Â Â Â Â Â  Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  Screening Due\n",
      "( Open check Required)\n",
      "Thanks & Regards\n",
      "MUHAMMED ANSAR\n",
      "Cargo Agent\n",
      "Road Feeder Service (RFS)\n",
      "P.O.Box 686, Dubai, United Arab Emirates\n",
      "TÂ +971 (0)4 6064382\n",
      "M\n",
      "+971 (0)561772122 |\n",
      "dnata.com\n",
      "|\n",
      "in\n",
      "BUSINESS DOCUMENT This document is intended for business use and should be distributed to intended recipients only.\n",
      "From:\n",
      "Sahadevan Sangeeth Tharayapurath [mailto:STharayapurath@etihad.ae]\n",
      "On Behalf Of\n",
      "Cargo Handling DXB\n",
      "Sent:\n",
      "11 February 2024 08:59\n",
      "To:\n",
      "Load Plan DXB <Loadplandxb@dnata.com>; dnata Cargo Operations DXB <dnatacargodxb@dnata.com>; DXB - Cargo RFS Operations <dxb.cargoRFS@dnata.com>; dnata Courier <dnatacourier@dnata.com>; dn ULD - WH & Trucking (OAL) <dnULDOALWT@dnata.com>; dn ULD\n",
      " - BGE & Empty (OAL) <dnULDOALBE@dnata.com>; dnata UCM & SCM <dnataUCM&SCM@dnata.com>\n",
      "Cc:\n",
      "'MICCO RFS (RFS@miccologistics.com)' <RFS@miccologistics.com>; EY - STN Officer Cgo <cargohandling_DXB@etihad.ae>; Shaheen Shafeek <SShafeek@etihad.ae>\n",
      "Subject:\n",
      "RFS LOAD PLAN : EY9557/ 11FEB2024 DXBAUH - VER: 1\n",
      "NOTE: This email originated from outside The Emirates Group. Please exercise caution when clicking on links or opening attachments.\n",
      "NOTE: PLS ATTACH EXPORT BILL OF ENTRY FOR EACH SHIPMENT ALONG WITH TRUCK MANIFEST COPY\n",
      "VERSIONÂ Â Â Â Â Â Â Â  : VER\n",
      "1\n",
      "FLT NO / DTEÂ Â Â  : EY95\n",
      "57\n",
      "/\n",
      "11FEB2024\n",
      "ORG/DESTÂ Â Â Â Â Â Â  : DXB/AUH\n",
      "ETD/ETAÂ Â Â Â Â Â Â Â  :\n",
      "233\n",
      "0LT / 023\n",
      "0\n",
      "LT\n",
      "ALLOCÂ Â Â Â Â Â Â Â Â Â  : 05PMC\n",
      "RFSÂ Â Â Â  Â Â Â Â Â Â Â  : MICCO\n",
      "ATTN FG\n",
      "5 / FG2\n",
      "AWB\n",
      "ORG\n",
      "DST\n",
      "DIM\n",
      "PCS\n",
      "WEIGHT(KG)\n",
      "VOL(CM)\n",
      "JOINING FLIGHT\n",
      "PRIORITY\n",
      "HANDLING CODE\n",
      "COMMODITY\n",
      "AGENT NAME\n",
      "REMARKS\n",
      "607-\n",
      "28027171\n",
      "DXB\n",
      "MLE\n",
      "1\n",
      "184\n",
      "1.71\n",
      "37\n",
      "EAP,GEN\n",
      "CHARGE AIR COOLERRADIATOR COOLERNOT RESTRICTED\n",
      "FAST LOGISTICS CARGO FZCO\n",
      "ATTN FG5 / DN ULD\n",
      "BELOW EMPTY UNITS PLANNED ON SUBJECT TRUCK ( ULD REQUEST FORM ATTACHED )\n",
      "ULD TYPE\n",
      "NUMBER OF ULD REQUIRED\n",
      "AKE\n",
      "08\n",
      "INSTRUCTIONS:\n",
      "-\n",
      "LOAD SR# 01 IN 1 PMC LD WITH PLASTIC\n",
      "-\n",
      "PLS REFER EY SEGREGATION CHART WHILE LOADING DG SHIPMENTS AND FOLLOW EY PROCEEDURE\n",
      "-\n",
      "PLS ENSURE TO CONDUCT ULD SERVICEABLITY CHECKS PRIOR TO BUILD-UP\n",
      "-\n",
      "CGO TO BE SECURED AS PER PROCEDURES\n",
      "-\n",
      "ENSURE CGO TO BE RDY AS PER STD\n",
      "-\n",
      "ENUSRE EU DESTINED ULDs ARE LOADED FIRST INTO THE TRUCK (FIRST LOAD & LAST OFFLOAD BASIS)\n",
      "ATTN MICCO,\n",
      "-\n",
      "PLS POS THE TRK ACC AND KEEP US INFRMD ABOUT TRK MVT X\n",
      "-\n",
      "PLS NOTE ABOVE TRUCK LOADING PRIORITY AND ADVISE DRIVERS ACCORDINGLY\n",
      "REGARDS\n",
      "SANGEETH\n",
      "EY CGO DXB & N.E.\n",
      "CONFIDENTIALITY / DISCLAIMER NOTICE:\n",
      "This e-mail and any attachments may contain confidential and privileged information. If you are not the intended recipient, please notify the sender immediately by return e-mail, delete this e-mail and destroy any copies. Any dissemination or use of this information\n",
      " by a person other than the intended recipient is unauthorised and may be illegal. Etihad Aviation Group or its employees are not responsible for any auto-generated spurious messages that you may receive from Etihad email addresses.\n",
      "http://www.etihad.com/\n",
      "CONFIDENTIALITY / DISCLAIMER NOTICE:\n",
      "This e-mail and any attachments may contain confidential and privileged information. If you are not the intended recipient, please notify the sender immediately by return e-mail, delete this e-mail and destroy any copies. Any dissemination or use of this information\n",
      " by a person other than the intended recipient is unauthorised and may be illegal. Etihad Aviation Group or its employees are not responsible for any auto-generated spurious messages that you may receive from Etihad email addresses.\n",
      "http://www.etihad.com/\n",
      "######################################################\n",
      " You are assisting the Airline Cargo team in extracting specific business-critical entities from customer emails. Each email consists of a subject and body, both delimited by triple backticks (```).\n",
      "\n",
      "You must return a JSON array of dictionaries, each representing one AWB (Air Waybill) entry with its corresponding entities. The structure should follow these rules:\n",
      "Return a JSON array of dictionaries.\n",
      "Each dictionary must include the extracted AWB and its associated entities.\n",
      "Omit any fields not found in the email; DO NOT assume values.\n",
      "\n",
      "The final output must be pure JSON: no explanations, no backticks, no extra text.\n",
      "The json output should appear as per the following format\n",
      "[\n",
      "    {\n",
      "        \"AWB\": \"\",\n",
      "        \"FlightNo\": \"\",\n",
      "        \"Departure-date\": \"\",\n",
      "        \"total-pieces\": ,\n",
      "        \"pieces@dimensions\": [\"\"],\n",
      "        \"dimension-unit\": [\"\"],\n",
      "        \"Weight\": ,\n",
      "        \"weight-unit\": \"\",\n",
      "        \"special-instruction\": \"\",\n",
      "        \"commodity-description\": \"\",\n",
      "        \"product-code\": \"\",\n",
      "        \"Source\": \"\",\n",
      "        \"Destination\": \"\"\n",
      "    }\n",
      "]\n",
      "AWB (Air Waybill):\n",
      "Must be 11-digit numbers starting with valid airline prefixes: <AWB_PREFIX>.\n",
      "May be referred to as \"MAWB\" or \"GUIA\".\n",
      "Remove hyphens or spaces.\n",
      "One dictionary per AWB; multiple AWBs = multiple dictionaries.\n",
      "\n",
      "FlightNo:\n",
      "Must start with valid carrier codes: <AIRLINE_PREFIX>.\n",
      "Format: airline code + number\n",
      "do not take the date value for the flight number if there is flight date attached with flight number (eg KE706/18APR in this only take KE706)\n",
      "if no values are found keep as null\n",
      "\n",
      "Departure-date:\n",
      "Extract in YYYY-MM-DD format.\n",
      "If given as a range like 23/24/07, choose the latest date (i.e., 2025-07-24).\n",
      "If given as a relative day (e.g., \"next Monday\"), assume today's date is 2025-03-22 (Saturday) and resolve accordingly.\n",
      "\n",
      "total-pieces:\n",
      "Integer value representing total cargo pieces.\n",
      "\n",
      "pieces@dimensions:\n",
      "Format: list like [\"2@24x17x9\"].\n",
      "May appear as pcs x l x b x h or pcs @ l x b x h.\n",
      "Extract all combinations; prioritize individual dimensions over total.\n",
      "\n",
      "dimension-unit:\n",
      "Supported units: \"CM\", \"M\", \"IN\", \"OTH\".\n",
      "Provide as a list matching the sequence of pieces@dimensions.\n",
      "\n",
      "Weight:\n",
      "If individual weights are given, compute the total.\n",
      "Use chargeable weight (CW) or gross weight (G/W) if explicitly mentioned.\n",
      "If weight is embedded in piece-dimension combos, extract accordingly.\n",
      "\n",
      "weight-unit:\n",
      "Supported values: \"KG\", \"KGS\", \"LBS\", \"OTH\".\n",
      "\n",
      "special-instruction:\n",
      "Extract any special handling notes.\n",
      "Always translate to English.\n",
      "\n",
      "commodity-description:\n",
      "Free text describing the goods.\n",
      "Always translate to English.\n",
      "\n",
      "product-code:\n",
      "If not explicitly given, infer from commodity description:\n",
      "\"GEN\" for general cargo\n",
      "\"HAZ\" for hazardous materials\n",
      "\"DG\" for dangerous goods\n",
      "\n",
      "Source / Destination:\n",
      "Extract from IATA codes in formats like EWR-OME (EWR = Source, OME = Destination).\n",
      "Do not assume source location from senderâ€™s location or from flight number.\n",
      "\n",
      "Must Translate all extracted text into  json.\n",
      "Do not fabricate missing values.\n",
      "Always return a clean JSON output only â€” no markdown, no backticks, no wrapping text.\n",
      "Must Not generate anything other than the base json file. Do NOT generate any code.\n",
      "Only the output is required do not generate anything else\n",
      "\n",
      "since there is a json format given generate just as the json format. Do not generate in loop. if it start to generate in loop stop the generation.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in dataloader:\n",
    "    prompts = batch[\"prompt\"]  # batch[\"prompt\"] is a list of strings\n",
    "    for prompt in prompts:\n",
    "        print(\"ðŸ“¨ Prompt:\\n\", prompt)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd7ab481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 128256])\n",
      "torch.Size([1, 512, 128256])\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "batch = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    t_out = teacher(**batch)\n",
    "s_out = student(**batch)\n",
    "print(t_out.logits.shape)  # [B, T, V]\n",
    "print(s_out.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ccb3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(student.parameters(), lr=5e-5)\n",
    "\n",
    "train_config = TrainingConfig(\n",
    "    device='cpu',                  # or 'cpu' if no GPU\n",
    "    output_dir='./saved_model',\n",
    "    log_dir='./log',\n",
    ")\n",
    "\n",
    "distill_config = DistillationConfig(\n",
    "    temperature=2.0,\n",
    "    kd_loss_type='ce',              # 'ce' for KLDiv, 'mse' for regression\n",
    "    kd_loss_weight=1.0,             # use 1.0 for pure distillation\n",
    "    hard_label_weight=1.0,          # set 0.0 to ignore ground-truth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ab25e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_adaptor(batch, model_output):\n",
    "    return {\n",
    "        \"logits\": model_output.logits,\n",
    "        \"labels\": batch.get(\"labels\", None) \n",
    "    }\n",
    "\n",
    "def batch_postprocessor(batch):\n",
    "    return {\n",
    "        \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "        \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "        \"labels\": batch[\"labels\"].to(device),\n",
    "    }\n",
    "def callback(step=None, loss=None, lr=None, model=None):\n",
    "    if loss is not None:\n",
    "        print(f\"[Step {step}] Total Loss: {loss:.4f} | LR: {lr:.6f}\")\n",
    "    else:\n",
    "        print(f\"[Step {step}] Loss is None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28cf84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "distiller = GeneralDistiller(\n",
    "    train_config=train_config,\n",
    "    distill_config=distill_config,\n",
    "    model_T=teacher,\n",
    "    model_S=student,\n",
    "    adaptor_T=get_adaptor,\n",
    "    adaptor_S=get_adaptor,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8abcd7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'textbrewer.distiller_general.GeneralDistiller'>\n"
     ]
    }
   ],
   "source": [
    "print(distiller.__class__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a8634bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_loader))\n",
    "# loss, loss_dict = distiller.train_on_batch(batch, {})\n",
    "# print(\"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1df2903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'textbrewer.distiller_general.GeneralDistiller'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DISTILL STEP] Loss: 9.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [07:29, 449.20s/it]\n",
      " 20%|â–ˆâ–ˆ        | 1/5 [07:29<29:56, 449.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 1] Loss is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DISTILL STEP] Loss: 10.1587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [06:36, 396.78s/it]\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [14:06<20:55, 418.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 2] Loss is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DISTILL STEP] Loss: 9.9698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [05:13, 313.59s/it]\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [19:19<12:21, 370.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 3] Loss is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DISTILL STEP] Loss: 9.7829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [05:36, 336.62s/it]\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [24:56<05:57, 357.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 4] Loss is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DISTILL STEP] Loss: 9.6582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [05:28, 328.36s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [30:24<00:00, 364.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step 5] Loss is None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(distiller.__class__)\n",
    "from types import MethodType\n",
    "\n",
    "def force_general_train(self, *args, **kwargs):\n",
    "    print(\"âœ… This is GeneralDistiller.train()\")\n",
    "    return GeneralDistiller.train(self, *args, **kwargs)\n",
    "\n",
    "distiller.train = MethodType(force_general_train, distiller)\n",
    "\n",
    "original_train_on_batch = distiller.train_on_batch\n",
    "\n",
    "def wrapped_train_on_batch(self, batch, args):\n",
    "    loss, loss_dict = original_train_on_batch(batch, args)\n",
    "    print(f\"[DISTILL STEP] Loss: {loss.item():.4f}\")\n",
    "    return loss, loss_dict\n",
    "\n",
    "from types import MethodType\n",
    "distiller.train_on_batch = MethodType(wrapped_train_on_batch, distiller)\n",
    "\n",
    "distiller.train_with_num_epochs(\n",
    "    optimizer=optimizer,\n",
    "    scheduler=None,\n",
    "    tqdm_disable=False,\n",
    "    dataloader=train_loader,\n",
    "    max_grad_norm=1.0,\n",
    "    num_epochs=5,\n",
    "    callback=callback,\n",
    "    batch_postprocessor=None\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b57dd37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\211369\\\\Desktop\\\\program\\\\distil\\\\distil2\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\211369\\\\Desktop\\\\program\\\\distil\\\\distil2\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\211369\\\\Desktop\\\\program\\\\distil\\\\distil2\\\\tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilled_model=r\"C:\\Users\\211369\\Desktop\\program\\distil\\distil2\"\n",
    "student.save_pretrained(distilled_model)\n",
    "tokenizer.save_pretrained(distilled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45a6bcad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'body' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     21\u001b[39m     body = msg.get_content()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Combine all into one string\u001b[39;00m\n\u001b[32m     24\u001b[39m email_string = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33mFrom: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfrom_\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33mTo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33mDate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[33mSubject: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     28\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbody\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'body' is not defined"
     ]
    }
   ],
   "source": [
    "import email\n",
    "from email import policy\n",
    "\n",
    "# Load the .eml file\n",
    "with open(r\"C:\\Users\\211369\\Desktop\\New folder (3)\\[External] (AR) NO SHOW   EY9557 _ 26JAN24_ DXBAUH - Cargo Handling DXB (cargohandling_DXB@etihad.ae) - 2024-01-26 2052.eml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    msg = email.message_from_file(f, policy=policy.default)\n",
    "\n",
    "# Extract parts\n",
    "subject = msg[\"subject\"]\n",
    "from_ = msg[\"from\"]\n",
    "to = msg[\"to\"]\n",
    "date = msg[\"date\"]\n",
    "\n",
    "# Extract body (handles plain or multipart)\n",
    "if msg.is_multipart():\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == \"text/plain\":\n",
    "            body = part.get_content()\n",
    "            break\n",
    "else:\n",
    "    body = msg.get_content()\n",
    "\n",
    "# Combine all into one string\n",
    "email_string = f\"\"\"From: {from_}\n",
    "To: {to}\n",
    "Date: {date}\n",
    "Subject: {subject}\n",
    "\n",
    "{body}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a2ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Path to your local model (e.g., distilled_model)\n",
    "model_path = r\"C:\\Users\\211369\\Desktop\\program\\distil\\distil_llm\"\n",
    "device = \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path).to(device)\n",
    "\n",
    "# Set pad token (important for causal models)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Input prompt\n",
    "prompt =  email_string\n",
    "\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=2000,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        top_p=0.5,\n",
    "        temperature=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode and print\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"ðŸ§  Model response:\\n\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
