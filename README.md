# distillation-of-llms
 Implementation of TextBrewer, a flexible knowledge distillation toolkit for NLP. Supports logits, hidden states, and attention-based distillation for transformer models, enabling efficient student model training with minimal accuracy loss. Compatible with Hugging Face Transformers.
